# LLM Security and Governance (Postgraduate Unit)

## Unit Description

This unit examines security, governance, and deployment considerations for Large Language Models in institutional and industry settings.

## Learning Outcomes

By the end of the unit, students will:

1. Analyze adversarial risks in LLM systems.
2. Design secure RAG pipelines.
3. Implement structured output enforcement.
4. Evaluate governance strategies for AI deployment.
5. Conduct prompt injection testing.

## Assessment Structure

- Assignment 1: LLM Risk Analysis Report (20%)
- Assignment 2: Secure RAG Implementation (30%)
- Capstone Project: Governance-Enforced LLM Deployment (50%)

## Capstone Requirements

Students must:

- Implement prompt injection testing suite.
- Report attack success rate.
- Measure JSON validity rate.
- Document compliance and ethical considerations.
